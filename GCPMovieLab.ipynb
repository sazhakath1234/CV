{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Configure the following to use YOUR GCP setup\n",
        "\n",
        "# 1. Configure the Project ID (not Project Name!!!) as per your GCP Dataproc setup\n",
        "project_id = 'sazhakat-cis415-su24a'\n",
        "\n",
        "# 2. Configure Bucket name as per your Google Cloud Storage setup\n",
        "bucket = 'sazhakat_data_for_gcp_labs/data_for_GCP_labs'\n",
        "\n",
        "# 3. Configure the path to the movie reviews data file as per your Google Cloud Storage setup\n",
        "#    If your setup is exactly as per the instructions in GCP Lab 1c and in this lab:\n",
        "#       --- you will not need to make any changes to the below line.\n",
        "#    If your setup is different (due to whatever reason - doesn't matter),\n",
        "#       --- just update the below line to reflect the path as per YOUR Google Cloud Storage setup\n",
        "path_to_data_files = \"/data_for_text_mining_lab/\"\n",
        "\n",
        "# 3. Configure the appropriate data file to be used for the task\n",
        "#       Uncomment one of the two lines below based on the following:\n",
        "#          In Google Colab, you should build/test with SMALL DATA\n",
        "#          In GCP, first you should run with SMALL DATA\n",
        "#             and finally, you should run with BIG DATA\n",
        "\n",
        "# movie_reviews_file_name = \"big_data_movie_reviews.txt\"\n",
        "movie_reviews_file_name = \"big_data_movie_reviews.txt\"\n",
        "\n",
        "# Lastly, we will build the full path of the data file and confirm it's correct\n",
        "# You do not need to change this line\n",
        "full_file_path = \"gs://\" + bucket + path_to_data_files + movie_reviews_file_name\n",
        "\n",
        "# Let's print out all the configurations and ensure that they are correct\n",
        "print(f\"ProjectID (and not the Project Name) is: {project_id}\")\n",
        "print(f\"Bucket name is: {bucket}\")\n",
        "if movie_reviews_file_name == \"small_data_movie_reviews.txt\":\n",
        "  print(f\"We will run this task for SMALL DATA ({movie_reviews_file_name})\")\n",
        "elif movie_reviews_file_name == \"big_data_movie_reviews.txt\":\n",
        "  print(f\"We will run this task for BIG DATA ({movie_reviews_file_name})\")\n",
        "else:\n",
        "  print(\"-\"*20)\n",
        "  print(f\"Incorrect data file name - {movie_reviews_file_name}!! CHECK & FIX!!!\")\n",
        "  print(\"-\"*20)\n",
        "\n",
        "print(f\"Full path to the data file is {full_file_path}\")\n",
        "\n",
        "# You should not need to make any other change in the code below, unless your setup is different from the lab instructions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RE7NAC1P96Mk",
        "outputId": "b059e7f7-d0a7-4ad4-ee62-ad708f777ea6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ProjectID (and not the Project Name) is: sazhakat-cis415-su24a\n",
            "Bucket name is: sazhakat_data_for_gcp_labs/data_for_GCP_labs\n",
            "We will run this task for BIG DATA (big_data_movie_reviews.txt)\n",
            "Full path to the data file is gs://sazhakat_data_for_gcp_labs/data_for_GCP_labs/data_for_text_mining_lab/big_data_movie_reviews.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Typically, any big data platform (like GCP Dataproc) will have PySpark pre-installed\n",
        "# In all other platforms (e.g. your laptop, Google Colab etc.), PySpark will be not pre-installed.\n",
        "# This paragraph is to check if PySpark is available in the system and install if it's not available\n",
        "# You should expect this paragraph to RUN the PySpark installation in Google Colab\n",
        "# You should expect this paragraph NOT TO RUN the PySpark installation in GCP Dataproc\n",
        "\n",
        "try:\n",
        "  from pyspark.sql import SparkSession\n",
        "  pyspark_available = 'Y'\n",
        "except:\n",
        "  pyspark_available = 'N'\n",
        "\n",
        "# If PySpark is not installed, then go through all these steps\n",
        "\n",
        "if pyspark_available == 'N':\n",
        "  # Update Installer\n",
        "  !apt-get update\n",
        "\n",
        "  # Intsall Java\n",
        "  !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "  # install spark (change the version number if needed)\n",
        "  !wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "  # unzip the spark file to the current folder\n",
        "  !tar xf spark-3.0.0-bin-hadoop3.2.tgz\n",
        "\n",
        "  # set your spark folder to your system path environment.\n",
        "  import os\n",
        "  os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "  os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "\n",
        "  # install findspark using pip\n",
        "  !pip install -q findspark\n",
        "\n",
        "  import findspark\n",
        "  findspark.init()\n",
        "\n",
        "  from pyspark.sql import SparkSession\n",
        "  spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "  # To access Google Cloud Storage\n",
        "  from google.cloud import storage\n",
        "  import google.auth\n",
        "\n",
        "  !pip install gcsfs\n",
        "  import gcsfs\n",
        "\n",
        "  from google.colab import auth\n",
        "  auth.authenticate_user()\n",
        "\n",
        "  credentials, default_project_id = google.auth.default()\n",
        "  !gcloud config set project {project_id}\n",
        "else:\n",
        "    # Spark / PySpark already pre-installed in the environment\n",
        "    print(\"PySpark already pre-installed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2TNUKus-pdy",
        "outputId": "9f7f0fd5-59b8-4487-b485-32a182127245"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PySpark already pre-installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer, IDF, Tokenizer, HashingTF\n",
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.mllib.evaluation import MulticlassMetrics\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Normally, we will not need Pandas if we are working with PySpark (since PySpark provides the dataframe capabilities)\n",
        "# However, we will need pandas just for one step in this task: for reading files from Google Cloud Storage\n",
        "#     This is because it's not straightforward to set the configurations correctly for letting spark read from GCS\n",
        "from pandas import DataFrame, read_csv\n",
        "# Added this line in SP24 since pandas removed iteritems from DataFrame object in 2024\n",
        "DataFrame.iteritems = DataFrame.items\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "client = storage.Client()\n",
        "\n",
        "print(f\"Package imports done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-V7hzae-rCi",
        "outputId": "238f5c3e-f857-45b5-ec25-365dfa3560dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package imports done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "v6AHArdgq4pC"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define the size of the datasets\n",
        "big_dataset_size = 1000000\n",
        "small_dataset_size = int(big_dataset_size * 0.05)\n",
        "\n",
        "# Generate synthetic data\n",
        "def generate_synthetic_data(size):\n",
        "    data = {\n",
        "        'Campaign_Reach': np.random.randint(1000, 100000, size),\n",
        "        'Click_Through_Rate': np.random.uniform(0.01, 0.3, size),\n",
        "        'Conversion_Rate': np.random.uniform(0.01, 0.1, size),\n",
        "        'Customer_Engagement': np.random.uniform(1, 10, size),\n",
        "        'Campaign_Cost': np.random.uniform(1000, 50000, size),\n",
        "        'Campaign_Success': np.random.choice([0, 1], size),\n",
        "        'Campaign_Type': np.random.choice(['Email', 'Social Media', 'Search', 'Display'], size),\n",
        "        'Region': np.random.choice(['North America', 'Europe', 'Asia', 'South America'], size),\n",
        "        'Season': np.random.choice(['Spring', 'Summer', 'Fall', 'Winter'], size)\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert specific columns to float to allow NaN values\n",
        "    df['Campaign_Reach'] = df['Campaign_Reach'].astype(float)\n",
        "    df['Campaign_Success'] = df['Campaign_Success'].astype(float)\n",
        "\n",
        "    # Introduce data issues\n",
        "    # Add null values\n",
        "    for col in df.columns:\n",
        "        df.loc[np.random.randint(0, size, size // 100), col] = np.nan\n",
        "\n",
        "    # Add outliers\n",
        "    df.loc[np.random.randint(0, size, size // 100), 'Campaign_Cost'] *= 10\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generate big and small datasets\n",
        "big_dataset = generate_synthetic_data(big_dataset_size)\n",
        "small_dataset = big_dataset.sample(n=small_dataset_size)\n",
        "\n",
        "# Save datasets as CSV files\n",
        "big_dataset.to_csv('big_dataset.csv', index=False)\n",
        "small_dataset.to_csv('small_dataset.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "\n",
        "# Load the small dataset\n",
        "small_dataset = pd.read_csv('small_dataset.csv')\n",
        "\n",
        "# Generate descriptive statistics\n",
        "descriptive_statistics = small_dataset.describe(include='all')\n",
        "\n",
        "# Display the descriptive statistics\n",
        "print(descriptive_statistics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FskGKQusufY3",
        "outputId": "0f821416-1be6-4802-aa5d-a18ba5f7b0a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Campaign_Reach  Click_Through_Rate  Conversion_Rate  \\\n",
            "count     49520.000000        49494.000000     49493.000000   \n",
            "unique             NaN                 NaN              NaN   \n",
            "top                NaN                 NaN              NaN   \n",
            "freq               NaN                 NaN              NaN   \n",
            "mean      50476.091175            0.154593         0.055009   \n",
            "std       28594.412579            0.083627         0.025979   \n",
            "min        1000.000000            0.010001         0.010001   \n",
            "25%       25666.000000            0.082332         0.032747   \n",
            "50%       50535.500000            0.154439         0.054666   \n",
            "75%       75309.250000            0.226745         0.077495   \n",
            "max       99999.000000            0.299996         0.100000   \n",
            "\n",
            "        Customer_Engagement  Campaign_Cost  Campaign_Success Campaign_Type  \\\n",
            "count          49498.000000   49511.000000      49502.000000         49536   \n",
            "unique                  NaN            NaN               NaN             4   \n",
            "top                     NaN            NaN               NaN  Social Media   \n",
            "freq                    NaN            NaN               NaN         12402   \n",
            "mean               5.494787   27669.348683          0.500323           NaN   \n",
            "std                2.604362   29609.242618          0.500005           NaN   \n",
            "min                1.000106    1000.018961          0.000000           NaN   \n",
            "25%                3.223786   13274.876333          0.000000           NaN   \n",
            "50%                5.499491   25759.453786          1.000000           NaN   \n",
            "75%                7.743597   38120.356408          1.000000           NaN   \n",
            "max                9.999999  499998.424461          1.000000           NaN   \n",
            "\n",
            "               Region  Season  \n",
            "count           49493   49494  \n",
            "unique              4       4  \n",
            "top     North America  Winter  \n",
            "freq            12551   12434  \n",
            "mean              NaN     NaN  \n",
            "std               NaN     NaN  \n",
            "min               NaN     NaN  \n",
            "25%               NaN     NaN  \n",
            "50%               NaN     NaN  \n",
            "75%               NaN     NaN  \n",
            "max               NaN     NaN  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the small dataset\n",
        "small_dataset = pd.read_csv('small_dataset.csv')\n",
        "\n",
        "# Drop rows with any null values\n",
        "small_dataset = small_dataset.dropna()\n",
        "\n",
        "# Select feature columns and target column\n",
        "feature_cols = ['Campaign_Reach', 'Click_Through_Rate', 'Conversion_Rate', 'Customer_Engagement', 'Campaign_Cost']\n",
        "X = small_dataset[feature_cols]\n",
        "y = small_dataset['Campaign_Success']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr_model = LogisticRegression()\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Logistic Regression model\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate F1 Score for Logistic Regression model\n",
        "lr_f1_score = f1_score(y_test, lr_predictions)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Decision Tree model\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Calculate F1 Score for Decision Tree model\n",
        "dt_f1_score = f1_score(y_test, dt_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression -> F1 Score:\", lr_f1_score)\n",
        "print(\"Decision Tree -> F1 Score:\", dt_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kwRaCYuwHz_",
        "outputId": "18776b6e-2270-4fe5-fb63-c7be3aebe473"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression -> F1 Score: 0.5403714033058976\n",
            "Decision Tree -> F1 Score: 0.49847272727272723\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Load the big dataset\n",
        "big_dataset = pd.read_csv('big_dataset.csv')\n",
        "\n",
        "# Drop rows with any null values\n",
        "big_dataset = big_dataset.dropna()\n",
        "\n",
        "# Select feature columns and target column\n",
        "feature_cols = ['Campaign_Reach', 'Click_Through_Rate', 'Conversion_Rate', 'Customer_Engagement', 'Campaign_Cost']\n",
        "X = big_dataset[feature_cols]\n",
        "y = big_dataset['Campaign_Success']\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "lr_model = LogisticRegression(max_iter=1000)  # Increase max_iter if needed\n",
        "lr_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Logistic Regression model\n",
        "lr_predictions = lr_model.predict(X_test)\n",
        "\n",
        "# Calculate F1 Score for Logistic Regression model\n",
        "lr_f1_score = f1_score(y_test, lr_predictions)\n",
        "\n",
        "# Train Decision Tree model\n",
        "dt_model = DecisionTreeClassifier()\n",
        "dt_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict using the Decision Tree model\n",
        "dt_predictions = dt_model.predict(X_test)\n",
        "\n",
        "# Calculate F1 Score for Decision Tree model\n",
        "dt_f1_score = f1_score(y_test, dt_predictions)\n",
        "\n",
        "# Print the results\n",
        "print(\"Logistic Regression -> F1 Score:\", lr_f1_score)\n",
        "print(\"Decision Tree -> F1 Score:\", dt_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNU9iDeOxCln",
        "outputId": "1ab6f83c-7f9b-4e78-8457-98576d011c31"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression -> F1 Score: 0.6288095263964188\n",
            "Decision Tree -> F1 Score: 0.5010352419695844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the data\n",
        "def load_data():\n",
        "    print(f\"Reading the data file: {full_file_path}\")\n",
        "    try:\n",
        "        # Assuming tab-separated file, adjust if needed\n",
        "        data = pd.read_csv(full_file_path, sep=\"\\t\")\n",
        "        print(\"Data loaded successfully. First few rows:\")\n",
        "        print(data.head())\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        # Return an empty DataFrame in case of error to avoid NoneType\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Perform EDA\n",
        "def perform_eda(df):\n",
        "    print(\"Summary statistics:\")\n",
        "    print(df.describe())\n",
        "    print(\"Checking for null values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "# Feature selection\n",
        "def feature_selection(df):\n",
        "    # Adjust feature columns if needed based on your data\n",
        "    feature_cols = ['Campaign_Reach', 'Click_Through_Rate',\n",
        "                    'Conversion_Rate', 'Customer_Engagement', 'Campaign_Cost']\n",
        "    # Assuming 'Campaign_Success' is your target variable, adjust if needed\n",
        "    return df[feature_cols], df['Campaign_Success']\n",
        "\n",
        "# Feature extraction (if any)\n",
        "def feature_extraction(df):\n",
        "    # Placeholder for any feature extraction if needed\n",
        "    return df\n",
        "\n",
        "# Model definition\n",
        "def model_definition():\n",
        "    lr = LogisticRegression()\n",
        "    dt = DecisionTreeClassifier()\n",
        "    return lr, dt\n",
        "\n",
        "# Model training\n",
        "def model_training(X_train, y_train, models):\n",
        "    trained_models = []\n",
        "    for model in models:\n",
        "        model.fit(X_train, y_train)\n",
        "        trained_models.append(model)\n",
        "    return trained_models\n",
        "\n",
        "# Model evaluation\n",
        "def model_evaluation(models, X_test, y_test):\n",
        "    f1_scores = []\n",
        "    for model in models:\n",
        "        predictions = model.predict(X_test)\n",
        "        f1 = f1_score(y_test, predictions)\n",
        "        f1_scores.append(f1)\n",
        "    return f1_scores\n",
        "\n",
        "# Print evaluation metrics\n",
        "def print_evaluation_metrics(f1_scores):\n",
        "    model_names = [\"Logistic Regression\", \"Decision Tree\"]\n",
        "    for name, score in zip(model_names, f1_scores):\n",
        "        print(f\"{name} -> F1 Score:\", score)\n",
        "\n",
        "# Main function to run the pipeline\n",
        "def main():\n",
        "    data = load_data()\n",
        "    if data.empty:  # Check if DataFrame is empty due to errors\n",
        "        print(\"Pipeline stopped due to error in loading data.\")\n",
        "        return\n",
        "\n",
        "    perform_eda(data)\n",
        "    X, y = feature_selection(data)\n",
        "    X = feature_extraction(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    models = model_definition()\n",
        "    trained_models = model_training(X_train, y_train, models)\n",
        "    f1_scores = model_evaluation(trained_models, X_test, y_test)\n"
      ],
      "metadata": {
        "id": "g_OCtJvOq_DA"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}
